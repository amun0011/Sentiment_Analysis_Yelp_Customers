{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis for Yelp Customer Reviews \n",
    "### Monash University\n",
    "### Author: Arunava Munshi\n",
    "### Date: June 6, 2019\n",
    "This project is about sentiment analysis of customer reviews of Yelp. In order to do this the following datasets have been provided -\n",
    "1. **train data.csv:** trn id and review text. Contains 650,000 product reviews which acts as the training data.\n",
    "2. **train label.csv:** trn id and sentiment labels. The label set (1,2,3,4,5) refer to positive polarity levels (strong negative, weak negative, neutral, weak positive, strong and positive) respectively.\n",
    "3. **test data.csv:** test id and review text. Contains 50,000 product reviews which acts as the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing required libraries\n",
    "The below libraries have been imported as part of the text pre-processing, feature selection and model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.util import ngrams\n",
    "import itertools\n",
    "from itertools import chain\n",
    "import multiprocessing as mp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import string\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "mpl.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Datasets\n",
    "The below code reads the required datasets.\n",
    "### Reading train_label.csv\n",
    "The below code is to load the training labels into dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_label.csv\n",
    "train_label_df = pd.read_csv(\"train_label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading train_data.csv\n",
    "The below code is to load the training reviews into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading train_data.csv\n",
    "train_data = pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Reviews and Labels\n",
    "Combining the training labels and reviews and removing the duplicate reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train_data and train_label\n",
    "train_data_set = pd.concat([train_data, train_label_df], axis=1)\n",
    "# Remove duplicate trn_id\n",
    "train_data_set = train_data_set.loc[:,~train_data_set.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading test_data.csv\n",
    "The below code is to load the test data reviews into dataframes and renames the column of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_set = pd.read_csv(\"test_data.csv\")\n",
    "test_data_set = test_data_set.rename(columns={'test_id': 'trn_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "Now we do three executions of selected model with the given data. We chose support vector machine as our final model for this project (For the reason, please look into the report). Each execution has the below steps.\n",
    "1. **Train Data Sampling:** Because this is a large dataset of more than 600000 reviews, we can't take all the data to build the model. This is not only memory centric, but also consumes more time. Rather we create samples of the input file.\n",
    "2. **Data Pre-processing:** After data sampling, the data preprocessing is done that includes various ways to clean the data into managable format.\n",
    "3. **Feature Selection:** The next step is feature selection. In this step, adequate features are selected for the sampled dataset.\n",
    "4. **Train/Test Split:** Because we do not have any labelled test data to check our accuracy, so we need to do train test split to understand the model accuracy.\n",
    "5. **Model Building:** This step builds the model on the training data from the train/test split and predict on test data from the same split.\n",
    "6. **Model Accuracy and Checking Model Accuracy:** Last, but not the least, this step checks the model accuracy for each execution after predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the execution cycle one with one set of data. \n",
    "### Train Data Sampling\n",
    "We aim to build a balanced dataset. We saw from our data exploration that the given review set is a balanced set and hence we need to generate balanced samples. So we are getting the data for each sentiment level first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataframe for each sentiment level\n",
    "train_data_set_1 = train_data_set[train_data_set['label'] == 1]\n",
    "train_data_set_2 = train_data_set[train_data_set['label'] == 2]\n",
    "train_data_set_3 = train_data_set[train_data_set['label'] == 3]\n",
    "train_data_set_4 = train_data_set[train_data_set['label'] == 4]\n",
    "train_data_set_5 = train_data_set[train_data_set['label'] == 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we are taking 5000 random samples from each sentiment level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random samples of data for each sentiment type\n",
    "train_data_set_1 = train_data_set_1.sample(n = 5000)\n",
    "train_data_set_2 = train_data_set_2.sample(n = 5000)\n",
    "train_data_set_3 = train_data_set_3.sample(n = 5000)\n",
    "train_data_set_4 = train_data_set_4.sample(n = 5000)\n",
    "train_data_set_5 = train_data_set_5.sample(n = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we merge all these individual samples to create the main train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge these dataframes\n",
    "train_data_set = pd.concat([train_data_set_1, train_data_set_2, train_data_set_3, train_data_set_4, train_data_set_5], axis=0)\n",
    "train_data_set = train_data_set.sample(frac=1)\n",
    "train_data_label = train_data_set['label']\n",
    "train_data_set = pd.concat([train_data_set[['trn_id', 'text']], test_data_set], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "We do the very basic data pre-processing in our first execusion. These steps include -\n",
    "1. **Emoticon handling:** Finding emoticons into the reviews and replacing them with correct vocabulary.\n",
    "2. **Word contraction:** Expanding the contracted words such as \"won't\" or \"can't\" into \"will not\" or \"cannot\".\n",
    "3. **Case normalization:** Lowercasing the respective texts.\n",
    "4. **Word tokenization:** Seperating the words into word tokens.\n",
    "5. **Digit removals:** Removing the digits.\n",
    "6. **Removing lemmatized words with lemmatization with POS tagging:** Removing lemmatized words after doing the Parts-OfpSpeech tagging to them.\n",
    "7. **Emphasis words:** Replacing the words with repeated characters.\n",
    "9. **Punctuation removal:** Removing the punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for handling Emoticons\n",
    "def handlingEmoticons(each_review):\n",
    "    each_review = re.sub(\"X-\\(\", ' angry ', each_review)\n",
    "    each_review = re.sub(\"</3\", ' broken heart ', each_review)\n",
    "    each_review = re.sub(\"O.o\", ' confused ', each_review)\n",
    "    each_review = re.sub(\"B-\\)\", ' angry ', each_review)\n",
    "    each_review = re.sub(\":_\\(|:'\\(\", ' crying ', each_review)\n",
    "    each_review = re.sub(\"\\\\\\:D/\", ' dancing ', each_review)\n",
    "    each_review = re.sub(\"\\*-\\*\", ' dazed ', each_review)\n",
    "    each_review = re.sub(\"=P|:-P|:P\", ' tongue out ', each_review)\n",
    "    each_review = re.sub(\"=\\)|:-\\)|:\\)|\\(-:\", ' happy ', each_review)\n",
    "    each_review = re.sub(\"<3\", ' heart ', each_review)\n",
    "    each_review = re.sub(\"{}\", ' hug ', each_review)\n",
    "    each_review = re.sub(\":-\\|\", ' indifferent ', each_review)\n",
    "    each_review = re.sub(\"X-p\", ' joking ', each_review)\n",
    "    each_review = re.sub(\"XD|=D\", ' laughing ', each_review)\n",
    "    each_review = re.sub(\"\\)-:|:-\\(|:\\(|=\\(\", ' sad ', each_review)\n",
    "    each_review = re.sub(\"=/\", ' mad ', each_review)\n",
    "    each_review = re.sub(\":-B\", ' nerd ', each_review)\n",
    "    each_review = re.sub(\"\\^_\\^\", ' overjoyed ', each_review)\n",
    "    each_review = re.sub(\":-/\", ' perplexed ', each_review)\n",
    "    each_review = re.sub(\":S\", ' sarcastic ', each_review)\n",
    "    each_review = re.sub(\":o|:O|:0|=O|:-o\", ' surprised ', each_review)\n",
    "    each_review = re.sub(\":-J\", ' tongue in cheek ', each_review)\n",
    "    each_review = re.sub(\":-\\\\\\\\\", ' undecided ', each_review)\n",
    "    each_review = re.sub(\"=D|:D|:-D\", ' very happy ', each_review)\n",
    "    each_review = re.sub(\";-\\)|;\\)\", ' winking ', each_review)\n",
    "    each_review = re.sub(\"\\|-O\", ' yawn ', each_review)\n",
    "    return(each_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Contraction word dictioary\n",
    "contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting english vocabulary for POS tagging\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "# Function to identify Parts-of-speech\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "def tokenizeWordTrain(text):\n",
    "    # Emoticon handling\n",
    "    text = handlingEmoticons(text)\n",
    "    # Case normalization and Punctuation Removal\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    # Word contraction\n",
    "    for word in text.split():\n",
    "        if word.lower() in contractions:\n",
    "            text = text.replace(word, contractions[word.lower()])\n",
    "    # Word tokenization\n",
    "    tokens = re.split('\\W+', text)\n",
    "    #Removing digits\n",
    "    tokens= [word for word in tokens if word.isalpha()]\n",
    "    # Removing lemmatized words with lemmatization with POS tagging\n",
    "    pos_tagged_word_tokens = pos_tag(tokens)\n",
    "    text = \" \".join([lemmatizer.lemmatize(each_item[0]) if get_wordnet_pos(each_item[1]) is None else lemmatizer.lemmatize(each_item[0], get_wordnet_pos(each_item[1])) for each_item in pos_tagged_word_tokens])\n",
    "    # Handling emphasis words\n",
    "    emhasis_regex = re.compile(r'\\s*\\b(?=[a-z\\d]*([a-z\\d])\\1{3}|\\d+\\b)[a-z\\d]+', re.IGNORECASE)\n",
    "    text = emhasis_regex.sub(\" emphasis \", text).strip()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing multiprocessing, using a pool of 10 process for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a pool of 30 processes in multiprocessing\n",
    "pool = mp.Pool(processes=30)  \n",
    "train_data_set['text'] = pool.map(tokenizeWordTrain, train_data_set['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do some analysis of the sample review corpus. We can see that the vocabulary size 47334, while the total number of tokens is 3042634 and more importantly the lexical dicersity is 64%, which indicates that our sampling is pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  86742 \n",
      "Total number of tokens:  9148752 \n",
      "Lexical diversity:  105.47084457356299\n"
     ]
    }
   ],
   "source": [
    "# To convert the dataframe into dictionary\n",
    "train_data_set_trn_id = train_data_set['trn_id'].tolist()\n",
    "train_data_set_text = train_data_set['text'].tolist()\n",
    "train_data_set_dict = dict(zip(train_data_set_trn_id, train_data_set_text))\n",
    "for k, v in train_data_set_dict.items():\n",
    "    train_data_set_dict[k] = re.split('\\W+', train_data_set_dict[k])\n",
    "\n",
    "# Calculating Lexical Diversity\n",
    "words = list(chain.from_iterable(train_data_set_dict.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print (\"Vocabulary size: \",len(vocab),\"\\nTotal number of tokens: \", len(words), \\\n",
    "\"\\nLexical diversity: \", lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "Now we do the feature selection from this preprocessed text. We use the tf-idf vectorizer for this feature selection.  We saw that, with this technique we could create 1998 features. In Feature selection we do the following -\n",
    "1. Unigram and Bigram Features\n",
    "2. We discarded any feature which occurs within less than 0.01% documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067455</td>\n",
       "      <td>0.067771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.043287</td>\n",
       "      <td>0.060176</td>\n",
       "      <td>0.08005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2009 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0         1         2     3         4     5     6     7     8     9     \\\n",
       "0  0.000000  0.000000  0.000000   0.0  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "1  0.067455  0.067771  0.000000   0.0  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "2  0.000000  0.000000  0.034965   0.0  0.072645   0.0   0.0   0.0   0.0   0.0   \n",
       "3  0.000000  0.000000  0.000000   0.0  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "4  0.000000  0.000000  0.000000   0.0  0.000000   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   ...  1999      2000  2001  2002  2003  2004      2005      2006     2007  \\\n",
       "0  ...   0.0  0.000000   0.0   0.0   0.0   0.0  0.000000  0.000000  0.00000   \n",
       "1  ...   0.0  0.067374   0.0   0.0   0.0   0.0  0.043287  0.060176  0.08005   \n",
       "2  ...   0.0  0.000000   0.0   0.0   0.0   0.0  0.000000  0.000000  0.00000   \n",
       "3  ...   0.0  0.000000   0.0   0.0   0.0   0.0  0.000000  0.000000  0.00000   \n",
       "4  ...   0.0  0.000000   0.0   0.0   0.0   0.0  0.000000  0.000000  0.00000   \n",
       "\n",
       "   2008  \n",
       "0   0.0  \n",
       "1   0.0  \n",
       "2   0.0  \n",
       "3   0.0  \n",
       "4   0.0  \n",
       "\n",
       "[5 rows x 2009 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = tfidf_vectorizer = TfidfVectorizer(min_df = 0.01, ngram_range=(1,2))\n",
    "tfidf = tfidf_vectorizer.fit_transform(train_data_set['text'])\n",
    "X_features = pd.DataFrame(tfidf.toarray())\n",
    "X_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split\n",
    "Now the next step is train test split. We split the train data and test data depending on the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_features.head(25000)\n",
    "X_test = X_features.tail(50000)\n",
    "y_train = train_data_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "The below code builds classification model for this train test split. We are using LinearSVM as our final model (Explanation for it). This time we use simple LinearSVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_model = LinearSVC(random_state=0, tol=1e-5, multi_class = \"ovr\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Accuracy and Checking Model Accuracy\n",
    "After the model building is done we do the prediction on test split of the train data and check the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_model_predictions = SVM_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model accuracy is calculated in keggle which is 0.55686."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pd.concat([test_data_set['trn_id'], pd.DataFrame(SVM_model_predictions)], axis=1)\n",
    "test_predictions = test_predictions.rename(columns={'trn_id': 'test_id', 0: 'label'})\n",
    "test_predictions.to_csv('predict_label.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python\n",
    "2. https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
